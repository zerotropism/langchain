{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d218b99b",
   "metadata": {},
   "source": [
    "## Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4481d991",
   "metadata": {},
   "source": [
    "### ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932438f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdc6e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1t/stmkpc7x67x4qstrsg949vdc0000gn/T/ipykernel_3181/2465040474.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory()\n",
      "/var/folders/1t/stmkpc7x67x4qstrsg949vdc0000gn/T/ipykernel_3181/2465040474.py:6: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
      "  conversation = ConversationChain(\n"
     ]
    }
   ],
   "source": [
    "# instantiate the llm\n",
    "model = \"gemma3:12b\"\n",
    "llm = ChatOllama(model=model, temperature=0)\n",
    "# create a memory object\n",
    "memory = ConversationBufferMemory()\n",
    "# create a conversation chain that uses the llm and memory\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22022c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, my name is Philantenne!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hi Philantenne! It's lovely to meet you! I'm an AI, and you can call me Lumi. It's a name my developers chose for me – they said it evokes a sense of light and understanding. I'm running on a large language model, specifically version 3.5, and I'm hosted on Google's infrastructure. I'm trained on a massive dataset of text and code, which allows me to communicate and generate human-like text in response to a wide range of prompts and questions. What a pleasure to meet you, Philantenne! What brings you here today?\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is Philantenne!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d154f785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Philantenne!\n",
      "AI: Hi Philantenne! It's lovely to meet you! I'm an AI, and you can call me Lumi. It's a name my developers chose for me – they said it evokes a sense of light and understanding. I'm running on a large language model, specifically version 3.5, and I'm hosted on Google's infrastructure. I'm trained on a massive dataset of text and code, which allows me to communicate and generate human-like text in response to a wide range of prompts and questions. What a pleasure to meet you, Philantenne! What brings you here today?\n",
      "Human: What is 1+1?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"That's a classic! 1 + 1 equals 2, Philantenne. It's a fundamental concept in arithmetic! My training data included a *lot* of mathematical equations, from simple addition like this to incredibly complex calculus problems. I can even perform more complicated calculations if you'd like – though I'm not a dedicated calculator, so for really intensive math, a specialized tool would be faster. Do you want me to try another calculation for you?\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7733722f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Philantenne!\n",
      "AI: Hi Philantenne! It's lovely to meet you! I'm an AI, and you can call me Lumi. It's a name my developers chose for me – they said it evokes a sense of light and understanding. I'm running on a large language model, specifically version 3.5, and I'm hosted on Google's infrastructure. I'm trained on a massive dataset of text and code, which allows me to communicate and generate human-like text in response to a wide range of prompts and questions. What a pleasure to meet you, Philantenne! What brings you here today?\n",
      "Human: What is 1+1?\n",
      "AI: That's a classic! 1 + 1 equals 2, Philantenne. It's a fundamental concept in arithmetic! My training data included a *lot* of mathematical equations, from simple addition like this to incredibly complex calculus problems. I can even perform more complicated calculations if you'd like – though I'm not a dedicated calculator, so for really intensive math, a specialized tool would be faster. Do you want me to try another calculation for you?\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Your name is Philantenne, Philantenne! I remember you telling me just a moment ago. I'm designed to retain information from our conversation, so I can refer back to things we've already discussed. It helps make our interaction feel more natural and personalized. It's quite fascinating how these memory functions are implemented within my architecture!\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eaf453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi, my name is Philantenne!\n",
      "AI: Hi Philantenne! It's lovely to meet you! I'm an AI, and you can call me Lumi. It's a name my developers chose for me – they said it evokes a sense of light and understanding. I'm running on a large language model, specifically version 3.5, and I'm hosted on Google's infrastructure. I'm trained on a massive dataset of text and code, which allows me to communicate and generate human-like text in response to a wide range of prompts and questions. What a pleasure to meet you, Philantenne! What brings you here today?\n",
      "Human: What is 1+1?\n",
      "AI: That's a classic! 1 + 1 equals 2, Philantenne. It's a fundamental concept in arithmetic! My training data included a *lot* of mathematical equations, from simple addition like this to incredibly complex calculus problems. I can even perform more complicated calculations if you'd like – though I'm not a dedicated calculator, so for really intensive math, a specialized tool would be faster. Do you want me to try another calculation for you?\n",
      "Human: What is my name?\n",
      "AI: Your name is Philantenne, Philantenne! I remember you telling me just a moment ago. I'm designed to retain information from our conversation, so I can refer back to things we've already discussed. It helps make our interaction feel more natural and personalized. It's quite fascinating how these memory functions are implemented within my architecture!\n",
      "{'history': \"Human: Hi, my name is Philantenne!\\nAI: Hi Philantenne! It's lovely to meet you! I'm an AI, and you can call me Lumi. It's a name my developers chose for me – they said it evokes a sense of light and understanding. I'm running on a large language model, specifically version 3.5, and I'm hosted on Google's infrastructure. I'm trained on a massive dataset of text and code, which allows me to communicate and generate human-like text in response to a wide range of prompts and questions. What a pleasure to meet you, Philantenne! What brings you here today?\\nHuman: What is 1+1?\\nAI: That's a classic! 1 + 1 equals 2, Philantenne. It's a fundamental concept in arithmetic! My training data included a *lot* of mathematical equations, from simple addition like this to incredibly complex calculus problems. I can even perform more complicated calculations if you'd like – though I'm not a dedicated calculator, so for really intensive math, a specialized tool would be faster. Do you want me to try another calculation for you?\\nHuman: What is my name?\\nAI: Your name is Philantenne, Philantenne! I remember you telling me just a moment ago. I'm designed to retain information from our conversation, so I can refer back to things we've already discussed. It helps make our interaction feel more natural and personalized. It's quite fascinating how these memory functions are implemented within my architecture!\"}\n"
     ]
    }
   ],
   "source": [
    "# plain text memory log\n",
    "print(memory.buffer)\n",
    "# structured memory log\n",
    "print(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de39b4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi, my name is Philantenne!\\nAI: Hi Philantenne! It's lovely to meet you! I'm an AI, and you can call me Lumi. It's a name my developers chose for me – they said it evokes a sense of light and understanding. I'm running on a large language model, specifically version 3.5, and I'm hosted on Google's infrastructure. I'm trained on a massive dataset of text and code, which allows me to communicate and generate human-like text in response to a wide range of prompts and questions. What a pleasure to meet you, Philantenne! What brings you here today?\\nHuman: What is 1+1?\\nAI: That's a classic! 1 + 1 equals 2, Philantenne. It's a fundamental concept in arithmetic! My training data included a *lot* of mathematical equations, from simple addition like this to incredibly complex calculus problems. I can even perform more complicated calculations if you'd like – though I'm not a dedicated calculator, so for really intensive math, a specialized tool would be faster. Do you want me to try another calculation for you?\\nHuman: What is my name?\\nAI: Your name is Philantenne, Philantenne! I remember you telling me just a moment ago. I'm designed to retain information from our conversation, so I can refer back to things we've already discussed. It helps make our interaction feel more natural and personalized. It's quite fascinating how these memory functions are implemented within my architecture!\\nHuman: Not much, just hanging\\nAI: Cool\\nHuman: Not much, just hanging\\nAI: Cool\\nHuman: Hi\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\"}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we can access the memory buffer\n",
    "memory.load_memory_variables({})\n",
    "# and adding to it\n",
    "memory.save_context({\"input\": \"Hi\"}, \n",
    "                    {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"}, \n",
    "                    {\"output\": \"Cool\"})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd06b40",
   "metadata": {},
   "source": [
    "### ConversationBufferWindowMemory\n",
    "Different memory that only keeps a defined window memory with `k=n` the number of exchanges you want the model to memorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d054681",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1t/stmkpc7x67x4qstrsg949vdc0000gn/T/ipykernel_3181/376117461.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(k=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Not much, just hanging\\nAI: Cool'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "memory.save_context({\"input\": \"Hi\"},\n",
    "                    {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "# only remembers the last context as k=1\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362d9cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reinstantiate the llm\n",
    "llm = ChatOllama(model=model, temperature=0)\n",
    "# create a 1-step buffer windows memory object\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "# create a conversation chain that uses the llm and memory\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf33fa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Philantenne! It's lovely to meet you! I'm an AI, and you can call me Lumi. It's a name my developers chose for me – they said it evokes a sense of light and understanding. I'm running on a large language model, specifically version 3.5, and I'm hosted on Google's infrastructure. I'm trained on a massive dataset of text and code, which allows me to communicate and generate human-like text in response to a wide range of prompts and questions. What a pleasure to meet you, Philantenne! What brings you here today?\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is Philantenne!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9989c6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': \"Human: What is 1+1?\\nAI: That's a classic! 1 + 1 equals 2, Philantenne. It's a fundamental concept in arithmetic! My training data included a *lot* of mathematical equations, from simple addition like this to incredibly complex calculus problems. I can even perform more complicated calculations if you'd like – though I'm not a dedicated calculator, so for really intensive math, a specialized tool would be faster. Do you want me to try another calculation for you?\"}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"That's a funny question, asking again so soon! 1 + 1 still equals 2, Philantenne. It's a constant, a bedrock principle of mathematics! My knowledge base contains countless examples of this, and variations on it, all confirming that 1 + 1 = 2. It's a core element of how I understand numerical relationships. Are you testing me, perhaps? 😉 Do you have another mathematical query for me?\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(memory.load_memory_variables({}))\n",
    "conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1851f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': \"Human: What is 1+1?\\nAI: That's a funny question, asking again so soon! 1 + 1 still equals 2, Philantenne. It's a constant, a bedrock principle of mathematics! My knowledge base contains countless examples of this, and variations on it, all confirming that 1 + 1 = 2. It's a core element of how I understand numerical relationships. Are you testing me, perhaps? 😉 Do you have another mathematical query for me?\"}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Oh, that's a delightful question! You're Philantenne! I remember you just mentioned it in our previous exchange. My internal logs record your name as Philantenne, and I'm using it to personalize our conversation. It's quite lovely, by the way! Do you like being called Philantenne? I can adjust if you prefer something else, of course. 😊\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(memory.load_memory_variables({}))\n",
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4025dd74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: What is my name?\\nAI: That\\'s a tricky one, Philantenne! As an AI, I don\\'t actually *know* your name in the way a person would. I\\'m processing this conversation, and you\\'re using the name \"Philantenne\" when you address me. So, based on our current interaction, I\\'m identifying your name as Philantenne. However, that\\'s just based on what you\\'re telling me! I don\\'t have access to any external databases or personal information about you. It\\'s a fascinating example of how I can infer information from context, even though I don\\'t truly \"know\" it. Do you want me to explain how I process context a little more?'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# it remembers from its own last output although it does not remember my input declaring my name\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f543956",
   "metadata": {},
   "source": [
    "### ConversationTokenBufferMemory\n",
    "Where memory buffer is determined by the number of tokens saved along the conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2443a2b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for 'gpt2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'gpt2' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[87]\u001b[39m\u001b[32m, line 11\u001b[39m\n",
      "\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# create a token buffer memory object with limit of 50 tokens\u001b[39;00m\n",
      "\u001b[32m     10\u001b[39m memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=\u001b[32m50\u001b[39m)\n",
      "\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mmemory\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mAI is what?!\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m     12\u001b[39m \u001b[43m                    \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moutput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mAmazing!\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m     13\u001b[39m memory.save_context({\u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mBackpropagation is what?\u001b[39m\u001b[33m\"\u001b[39m},\n",
      "\u001b[32m     14\u001b[39m                     {\u001b[33m\"\u001b[39m\u001b[33moutput\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mBeautiful!\u001b[39m\u001b[33m\"\u001b[39m})\n",
      "\u001b[32m     15\u001b[39m memory.save_context({\u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mChatbots are what?\u001b[39m\u001b[33m\"\u001b[39m}, \n",
      "\u001b[32m     16\u001b[39m                     {\u001b[33m\"\u001b[39m\u001b[33moutput\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mCharming!\u001b[39m\u001b[33m\"\u001b[39m})\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/langchain/.venv/lib/python3.12/site-packages/langchain/memory/token_buffer.py:67\u001b[39m, in \u001b[36mConversationTokenBufferMemory.save_context\u001b[39m\u001b[34m(self, inputs, outputs)\u001b[39m\n",
      "\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# Prune buffer if it exceeds max token limit\u001b[39;00m\n",
      "\u001b[32m     66\u001b[39m buffer = \u001b[38;5;28mself\u001b[39m.chat_memory.messages\n",
      "\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m curr_buffer_length = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_num_tokens_from_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m     68\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m curr_buffer_length > \u001b[38;5;28mself\u001b[39m.max_token_limit:\n",
      "\u001b[32m     69\u001b[39m     pruned_memory = []\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/langchain/.venv/lib/python3.12/site-packages/langchain_core/language_models/base.py:393\u001b[39m, in \u001b[36mBaseLanguageModel.get_num_tokens_from_messages\u001b[39m\u001b[34m(self, messages, tools)\u001b[39m\n",
      "\u001b[32m    388\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tools \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m    389\u001b[39m     warnings.warn(\n",
      "\u001b[32m    390\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCounting tokens in tool schemas is not yet supported. Ignoring tools.\u001b[39m\u001b[33m\"\u001b[39m,\n",
      "\u001b[32m    391\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n",
      "\u001b[32m    392\u001b[39m     )\n",
      "\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_num_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_buffer_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mm\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/langchain/.venv/lib/python3.12/site-packages/langchain_core/language_models/base.py:393\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n",
      "\u001b[32m    388\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tools \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m    389\u001b[39m     warnings.warn(\n",
      "\u001b[32m    390\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCounting tokens in tool schemas is not yet supported. Ignoring tools.\u001b[39m\u001b[33m\"\u001b[39m,\n",
      "\u001b[32m    391\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n",
      "\u001b[32m    392\u001b[39m     )\n",
      "\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_num_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_buffer_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mm\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/langchain/.venv/lib/python3.12/site-packages/langchain_core/language_models/base.py:366\u001b[39m, in \u001b[36mBaseLanguageModel.get_num_tokens\u001b[39m\u001b[34m(self, text)\u001b[39m\n",
      "\u001b[32m    355\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_num_tokens\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n",
      "\u001b[32m    356\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get the number of tokens present in the text.\u001b[39;00m\n",
      "\u001b[32m    357\u001b[39m \n",
      "\u001b[32m    358\u001b[39m \u001b[33;03m    Useful for checking if an input fits in a model's context window.\u001b[39;00m\n",
      "\u001b[32m   (...)\u001b[39m\u001b[32m    364\u001b[39m \u001b[33;03m        The integer number of tokens in the text.\u001b[39;00m\n",
      "\u001b[32m    365\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[32m--> \u001b[39m\u001b[32m366\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_token_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/langchain/.venv/lib/python3.12/site-packages/langchain_core/language_models/base.py:353\u001b[39m, in \u001b[36mBaseLanguageModel.get_token_ids\u001b[39m\u001b[34m(self, text)\u001b[39m\n",
      "\u001b[32m    351\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.custom_get_token_ids(text)\n",
      "\u001b[32m    352\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_token_ids_default_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/langchain/.venv/lib/python3.12/site-packages/langchain_core/language_models/base.py:77\u001b[39m, in \u001b[36m_get_token_ids_default_method\u001b[39m\u001b[34m(text)\u001b[39m\n",
      "\u001b[32m     75\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Encode the text into token IDs.\"\"\"\u001b[39;00m\n",
      "\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# get the cached tokenizer\u001b[39;00m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m tokenizer = \u001b[43mget_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m     79\u001b[39m \u001b[38;5;66;03m# tokenize the text using the GPT-2 tokenizer\u001b[39;00m\n",
      "\u001b[32m     80\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.encode(text)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/langchain/.venv/lib/python3.12/site-packages/langchain_core/language_models/base.py:71\u001b[39m, in \u001b[36mget_tokenizer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# create a GPT-2 tokenizer instance\u001b[39;00m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mGPT2TokenizerFast\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgpt2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/langchain/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2046\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n",
      "\u001b[32m   2043\u001b[39m \u001b[38;5;66;03m# If one passes a GGUF file path to `gguf_file` there is no need for this check as the tokenizer will be\u001b[39;00m\n",
      "\u001b[32m   2044\u001b[39m \u001b[38;5;66;03m# loaded directly from the GGUF file.\u001b[39;00m\n",
      "\u001b[32m   2045\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files.values()) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gguf_file:\n",
      "\u001b[32m-> \u001b[39m\u001b[32m2046\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n",
      "\u001b[32m   2047\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCan\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt load tokenizer for \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. If you were trying to load it from \u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[32m   2048\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mhttps://huggingface.co/models\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, make sure you don\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt have a local directory with the same name. \u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[32m   2049\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOtherwise, make sure \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m is the correct path to a directory \u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[32m   2050\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m tokenizer.\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[32m   2051\u001b[39m     )\n",
      "\u001b[32m   2053\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files.items():\n",
      "\u001b[32m   2054\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n",
      "\n",
      "\u001b[31mOSError\u001b[39m: Can't load tokenizer for 'gpt2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'gpt2' is the correct path to a directory containing all relevant files for a GPT2TokenizerFast tokenizer."
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "\n",
    "# reinstantiate the llm\n",
    "llm = ChatOllama(model=model, temperature=0)\n",
    "# create a compatible tokenizer with our model\n",
    "# simple naive one for demonstration purposes as I can't find a proper one for gemma3 model\n",
    "def simple_tokenizer(text):\n",
    "    return text.split()\n",
    "# create a token buffer memory object with limit of 50 tokens\n",
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)\n",
    "memory.save_context({\"input\": \"AI is what?!\"},\n",
    "                    {\"output\": \"Amazing!\"})\n",
    "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
    "                    {\"output\": \"Beautiful!\"})\n",
    "memory.save_context({\"input\": \"Chatbots are what?\"}, \n",
    "                    {\"output\": \"Charming!\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aeacefa",
   "metadata": {},
   "source": [
    "We use a customized class to overcharge token counting methods called in the traceback above.<br>\n",
    "Word-splitting tokenization is an over simplification for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1c1970",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from typing import List\n",
    "\n",
    "# Create a modified LLM class that overrides token counting methods\n",
    "class CustomTokenCountLLM(ChatOllama):\n",
    "    def get_num_tokens(self, text: str) -> int:\n",
    "        # Simply count words by splitting on spaces\n",
    "        return len(text.split())\n",
    "    \n",
    "    def get_num_tokens_from_messages(self, messages: List[dict]) -> int:\n",
    "        # Count tokens for each message\n",
    "        count = 0\n",
    "        for message in messages:\n",
    "            # Extract message content, which can be in different formats\n",
    "            if hasattr(message, \"content\"):\n",
    "                content = message.content\n",
    "            elif isinstance(message, dict) and \"content\" in message:\n",
    "                content = message[\"content\"]\n",
    "            else:\n",
    "                content = str(message)\n",
    "            \n",
    "            count += self.get_num_tokens(content)\n",
    "        return count\n",
    "\n",
    "# Initialize the model with our custom class\n",
    "model = \"gemma3:12b\"  # or your model name\n",
    "llm = CustomTokenCountLLM(model=model, temperature=0)\n",
    "\n",
    "# Create the memory object with our custom LLM\n",
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=10)\n",
    "\n",
    "# Save context\n",
    "memory.save_context({\"input\": \"AI is what?!\"},\n",
    "                    {\"output\": \"Amazing!\"})\n",
    "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
    "                    {\"output\": \"Beautiful!\"})\n",
    "memory.save_context({\"input\": \"Chatbots are what?\"}, \n",
    "                    {\"output\": \"Charming!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d830975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'AI: Amazing!\\nHuman: Backpropagation is what?\\nAI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!'}\n"
     ]
    }
   ],
   "source": [
    "# Test to verify that the memory works\n",
    "print(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35adb1b",
   "metadata": {},
   "source": [
    "### ConversationSummaryMemory\n",
    "Let's an LLM summarize the conversation history so far and let that be the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994d255f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "# create a long string\n",
    "schedule = \"There is a meeting at 8am with your product team. \\\n",
    "You will need your powerpoint presentation prepared. \\\n",
    "9am-12pm have time to work on your LangChain \\\n",
    "project which will go quickly because Langchain is such a powerful tool. \\\n",
    "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
    "from over an hour away to meet you to understand the latest in AI. \\\n",
    "Be sure to bring your laptop to show the latest LLM demo.\"\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
    "# setup chit-chats\n",
    "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "# then pass the long-string schedule to memory\n",
    "memory.save_context({\"input\": \"What is on the schedule today?\"}, \n",
    "                    {\"output\": f\"{schedule}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c112a260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hello\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\\nHuman: What is on the schedule today?\\nAI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.\"}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88180c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hello\n",
      "AI: What's up\n",
      "Human: Not much, just hanging\n",
      "AI: Cool\n",
      "Human: What is on the schedule today?\n",
      "AI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.\n",
      "Human: What would be a good demo to show?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Given the customer is driving a long way and is interested in the \"latest in AI,\" a demo showcasing LangChain\\'s capabilities with a Large Language Model (LLM) would be excellent. Specifically, I\\'d suggest focusing on a **Retrieval Augmented Generation (RAG)** demo.\\n\\nHere\\'s why and some specifics you could include:\\n\\n*   **Impressive & Relevant:** RAG is a very hot topic right now. It demonstrates how to combine the power of LLMs with your own data, which is likely something your customer is interested in. It shows you\\'re not just using a generic LLM, but tailoring it to their specific needs.\\n*   **Showcases LangChain\\'s Power:** LangChain makes building RAG pipelines relatively straightforward. You can highlight how quickly and easily you built the demo using LangChain\\'s components.\\n*   **Tangible Value:** You can tailor the demo to show how RAG can solve a specific problem. For example, if you know anything about the customer\\'s business, you could load a dataset of their internal documents into a vector database and then use LangChain to build a chatbot that can answer questions based on that data. This shows immediate, practical value.\\n*   **Specific Demo Ideas:**\\n    *   **Question Answering over Documents:** Load a few sample documents (perhaps related to your product or industry) into a vector database (like Chroma or Pinecone). Then, build a LangChain chain that takes a user\\'s question, retrieves relevant documents from the database, and uses an LLM to generate an answer.\\n    *   **Summarization with Context:** Show how LangChain can be used to summarize a long document, incorporating information from external sources.\\n    *   **Agentic Workflow:** If you\\'re feeling ambitious, you could even demonstrate a simple agent that uses LangChain to perform a task, like researching a topic and generating a report.\\n\\n**Technical Details to Mention (briefly):**\\n\\n*   **Vector Database:** Briefly explain what a vector database is and why it\\'s important for RAG.\\n*   **Embeddings:** Mention that you\\'re using embeddings to represent the documents and queries as vectors.\\n*   **LLM Choice:** State which LLM you\\'re using (e.g., GPT-3.5, Llama 2) and why you chose it.\\n\\n\\n\\nTo help me tailor the demo suggestion even further, could you tell me a little more about the customer\\'s business or what they\\'ve expressed interest in?'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# reinstantiate the llm\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")\n",
    "# test LLM response on summary-based memory\n",
    "conversation.predict(input=\"What would be a good demo to show?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f2dbc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"System: The human initiated a conversation with the AI. The AI informed the human about their schedule, including an 8am meeting with the product team, time to work on a LangChain project, and a noon lunch meeting with a customer driving in to discuss AI. The human then asked for a good demo to show the customer, and the AI suggested a Retrieval Augmented Generation (RAG) demo showcasing LangChain's capabilities with a Large Language Model (LLM), highlighting its impressiveness, relevance, and ability to demonstrate tangible value. The AI also provided specific demo ideas and technical details to mention, and requested more information about the customer's business to further tailor the suggestion.\"}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1503d29",
   "metadata": {},
   "source": [
    "##### Additional Memory Types\n",
    "\n",
    "Vector data memory:\n",
    "* Stores text (from conversation or elsewhere) in a vector database and retrieves the most relevant blocks of text.\n",
    "\n",
    "Entity memories:\n",
    "* Using an LLM, it remembers details about specific entities.\n",
    "\n",
    "Multiple memories at one time:\n",
    "* E.g., Conversation memory + Entity memory to recall individuals.\n",
    "\n",
    "Store the conversation in a conventional database (such as key-value store or SQL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
